{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/ubuntu/spark-3.0.1-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spam').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the dataset \n",
    "\n",
    "df = spark.read.csv('SMSSpamCollection',inferSchema=True,\n",
    "                   sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an optional step, we just rename our columns so that it's easier for us to work\n",
    "\n",
    "df = df.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We suspect that there is significant difference in the length between\n",
    "# the spam and ham emails. Thus we do some feature engineering and we \n",
    "# compute the length of each email. \n",
    "\n",
    "from pyspark.sql.functions import length, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('length',length(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thatÂ’s th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|       avg_length|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We see that there is significant difference between the length \n",
    "# of the spam and ham emails. \n",
    "\n",
    "df.groupBy('class').agg(mean('length').alias('avg_length')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a pipeline in order to prepare our data for training our model. We start by importing Natural Language Proccesing methods. The Tokenizer function is applied on the 'text' column and gives back a list consisting of the words of the corresponding email. Then the StopWordsRemover function is used to identify -and remove from the tokens list- words that appear frequenctly and they don't carry so much meaning. The CountVectorizer function is used to convert a collection of text documents, to vectors of token counts. The Inverse Document Frequency (IDF) function is applied on vectors and is a measure on how much information a term provides.\n",
    "\n",
    "Finally, we use the StringIndexer function in order to convert the categorical label 'ham' and 'spam', to numerical label. The VectorAssembler is a function converting the features into a single vector of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (Tokenizer, StopWordsRemover, CountVectorizer,\n",
    "                               IDF,StringIndexer,VectorAssembler)\n",
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text',outputCol='words')\n",
    "stop_remove = StopWordsRemover(inputCol='words',outputCol='final_words')\n",
    "count_vec = CountVectorizer(inputCol='final_words',outputCol='c_vec')\n",
    "idf = IDF(inputCol='c_vec',outputCol='tf_idf')\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages = [ham_spam_to_num,tokenizer,stop_remove,\n",
    "                                    count_vec,idf,clean_up])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to do some Machine Learning! We pick the Naive Bayes classification model, but in fact this choice is not unique. It's worth trying different models and compare them, based on some classification metrics such as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol='features',labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = clean_data.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "|  0.0|(13424,[0,70,80,1...|\n",
      "|  0.0|(13424,[36,134,31...|\n",
      "|  1.0|(13424,[10,60,139...|\n",
      "|  0.0|(13424,[10,53,103...|\n",
      "|  0.0|(13424,[125,184,4...|\n",
      "|  1.0|(13424,[1,47,118,...|\n",
      "|  1.0|(13424,[0,1,13,27...|\n",
      "|  0.0|(13424,[18,43,120...|\n",
      "|  1.0|(13424,[8,17,37,8...|\n",
      "|  1.0|(13424,[13,30,47,...|\n",
      "|  0.0|(13424,[39,96,217...|\n",
      "|  0.0|(13424,[552,1697,...|\n",
      "|  1.0|(13424,[30,109,11...|\n",
      "|  0.0|(13424,[82,214,47...|\n",
      "|  0.0|(13424,[0,2,49,13...|\n",
      "|  0.0|(13424,[0,74,105,...|\n",
      "|  1.0|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detector = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = spam_detector.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,13,...|[-627.60992036585...|[0.99999999999552...|       0.0|\n",
      "|  0.0|(13424,[0,1,3,9,1...|[-572.27143914660...|[1.0,1.5826959828...|       0.0|\n",
      "|  0.0|(13424,[0,1,5,20,...|[-803.84053843071...|[1.0,1.7834899535...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[-542.29078511032...|[1.0,1.7207497290...|       0.0|\n",
      "|  0.0|(13424,[0,1,18,20...|[-837.53409585006...|[1.0,4.5043123696...|       0.0|\n",
      "|  0.0|(13424,[0,1,24,31...|[-338.72017694265...|[1.0,3.2648930659...|       0.0|\n",
      "|  0.0|(13424,[0,1,27,88...|[-1535.5633153710...|[0.00147975093054...|       1.0|\n",
      "|  0.0|(13424,[0,1,31,43...|[-339.07700057359...|[1.0,9.0850525172...|       0.0|\n",
      "|  0.0|(13424,[0,1,43,69...|[-613.82115299856...|[0.99995872400822...|       0.0|\n",
      "|  0.0|(13424,[0,1,146,1...|[-250.52055991144...|[0.91913896238428...|       0.0|\n",
      "|  0.0|(13424,[0,1,498,5...|[-317.71146095617...|[0.99999999999972...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-97.638784136793...|[0.99999997757652...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,3...|[-484.58112838773...|[1.0,2.0277454778...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3284.8191692068...|  [1.0,1.49327E-318]|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-2479.5248258682...|[1.0,2.3408562996...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-1631.5089212398...|[1.0,3.2547549655...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,1...|[-1312.0156346323...|[1.0,2.9630468905...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,25,...|[-444.87906459492...|[1.0,1.9686821456...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,40,...|[-1585.1686205334...|[0.99999999999362...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,11,...|[-1342.4061220053...|[1.0,1.9806811095...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an instance of an evaluator, with the metric name being the 'accuracy'\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='label',\n",
    "                                            metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_acc = acc_eval.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes model is: 0.9177842565597668\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the Naive Bayes model is: {nb_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a logistic regression model\n",
    "\n",
    "lg = LogisticRegression(featuresCol='features',labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_model = lg.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_test_results = lg_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_acc = acc_eval.evaluate(lg_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic Regression model is: 0.970262390670554\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the Logistic Regression model is: {lg_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
